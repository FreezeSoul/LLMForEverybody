<p> 
<a href="https://github.com/luhengshiwo/LLMForEverybody/stargazers">
<img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/github.svg" > </a>
<a href="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/wechat.png"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/wechat.svg" > </a>
<a href="https://www.zhihu.com/people/lu-heng-45-95"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/zhihu.svg"> </a>
<a href="https://blog.csdn.net/qq_25295605?spm=1011.2415.3001.5343"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/csdn.svg"> </a>
</p> 


## 目录

- 🐱 [大模型训练之Pre-Training](#大模型训练之Pre-Training)
  - 🐼[aaa](#aaa)
- 🐶[大模型训练之post-training](#大模型训练之Post-Training)
  - 🐹[对齐](#对齐)
- 🐭[Prompt Engineering](#Prompt-Engineering)
- 🐯[大模型微调](#大模型微调)
- 🐻[大模型推理](#大模型推理)
- 🐨[大模型部署](#大模型部署)
- 🦁[大模型应用](#大模型应用)
- 🐘[显卡知识](#显卡知识)
- 🐳[时事热点](#时事热点)



## 大模型训练之Pre-Training

**[⬆ 一键返回目录](#目录)** 

### aaa

[什么是perplexity](url)

[Pre-Training预训练Llama-3.1 405B超大杯，需要多少算力资源？](url)

[MoE VS Dense](url)

### Attention

[看懂FlashAttention需要的数学储备是？高考数学最后一道大题](url)

[MHA, MQA, GQA](url)

## 大模型训练之Post-Training

**[⬆ 一键返回目录](#目录)**

### 对齐

[对齐策略：RLHF，PPO，DPO，ORPO](url)

## Prompt Engineering
**[⬆ 一键返回目录](#目录)**

[过去式就能越狱大模型？一文了解大模型安全攻防战！](url)

## 大模型微调

**[⬆ 一键返回目录](#目录)**

## 大模型推理

**[⬆ 一键返回目录](#目录)**

[K-V Cache](url)

[Prefilling](url)


## 大模型部署

**[⬆ 一键返回目录](#目录)**

## 大模型应用

**[⬆ 一键返回目录](#目录)**


## 显卡知识

**[⬆ 一键返回目录](#目录)**

[AGI时代人人都可以看懂的显卡知识](url)

## 时事热点

[Llama 3.1 405B 为什么这么大？](url)

[9.11大于9.9？大模型怎么又翻车了？](url)

[10分钟教你套壳（不是）Llama-3，小白也能上手](url)

