<p align="center">
  <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/pr/banner.jpg"" >
</p>

<p> 
<a href="https://github.com/luhengshiwo/LLMForEverybody/stargazers">
<img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/github.svg" > </a>
<a href="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/pr/wechat.png"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/wechat.svg" > </a>
<a href="https://www.zhihu.com/people/lu-heng-45-95"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/zhihu.svg"> </a>
<a href="https://blog.csdn.net/qq_25295605?spm=1011.2415.3001.5343"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/csdn.svg"> </a>
<a href="https://juejin.cn/user/3824524390049531"> <img src="https://github.com/luhengshiwo/LLMForEverybody/blob/main/pic/common/svg/juejin.svg"> </a>
</p> 


## 目录

- 🐱 [大模型训练之Pre-Training](#大模型训练之Pre-Training)
  - 🐭[Attention](#Attention)
  - 🐯[位置编码](#位置编码)
- 🐶[大模型训练之post-training](#大模型训练之Post-Training)
  - 🐹[对齐](#对齐)
- 🐭[Prompt Engineering](#Prompt-Engineering)
- 🐯[大模型微调](#大模型微调)
- 🐻[大模型推理](#大模型推理)
- 🐨[大模型部署](#大模型部署)
- 🦁[大模型应用](#大模型应用)
- 🐘[显卡知识](#显卡知识)
- 🐳[时事热点](#时事热点)



## 大模型训练之Pre-Training

**[⬆ 一键返回目录](#目录)** 


[什么是perplexity](https://mp.weixin.qq.com/s?__biz=MzkyOTY4Mjc4MQ==&mid=2247483766&idx=1&sn=56563281557b6f58feacb935eb6a872a&chksm=c2048544f5730c52cf2bf4c9ed60ac0a21793bacdddc4d63b481d4aa887bc6a838fecf0b6cc7&token=607452854&lang=zh_CN#rd)

[Pre-Training预训练Llama-3.1 405B超大杯，需要多少算力资源？](https://mp.weixin.qq.com/s?__biz=MzkyOTY4Mjc4MQ==&mid=2247483839&idx=1&sn=3f35dfe8ed2c87bf4c0b4ac7bfa3e6a9&chksm=c204858df5730c9b8a152a0330dee0183467a063c25aadd0da7cc47d9d5b2f97347fab22708d&token=607452854&lang=zh_CN#rd)

[MoE VS Dense (pending)](url)

### Attention

[FlashAttention V1](https://zhuanlan.zhihu.com/p/713048343)

[FlashAttention V2](https://blog.csdn.net/qq_25295605/article/details/141633955?spm=1001.2014.3001.5502)

[Attention机制: MHA, MQA, GQA](https://zhuanlan.zhihu.com/p/714323628)

### 位置编码

[复变函数在大模型位置编码中的应用](https://blog.csdn.net/qq_25295605/article/details/141708680)

[最美的数学公式-欧拉公式](https://blog.csdn.net/qq_25295605/article/details/141571407?spm=1001.2014.3001.5502)

[什么是大模型的位置编码Position Encoding?](https://blog.csdn.net/qq_25295605/article/details/141571042?spm=1001.2014.3001.5502)

[从欧拉公式的美到旋转位置编码RoPE](https://blog.csdn.net/qq_25295605/article/details/141630770?spm=1001.2014.3001.5502)

## 大模型训练之Post-Training

**[⬆ 一键返回目录](#目录)**

### 对齐

[对齐策略：RLHF，PPO，DPO，ORPO](https://zhuanlan.zhihu.com/p/713100677)

## Prompt Engineering
**[⬆ 一键返回目录](#目录)**

[过去式就能越狱大模型？一文了解大模型安全攻防战！](https://zhuanlan.zhihu.com/p/713100677)

[Prompt Engineering](https://zhuanlan.zhihu.com/p/713318025)

## 大模型微调

**[⬆ 一键返回目录](#目录)**

[10分钟教你套壳（不是）Llama-3，小白也能上手](https://mp.weixin.qq.com/s?__biz=MzkyOTY4Mjc4MQ==&mid=2247483895&idx=1&sn=72e9ca9874aeb4fd51a076c14341242f&chksm=c20485c5f5730cd38f43cf32cc851ade15286d5bd14c8107906449f8c52db9d3bfd72cfc40c8&token=607452854&lang=zh_CN#rd)


## 大模型推理

**[⬆ 一键返回目录](#目录)**

[K-V Cache & Prefilling](https://zhuanlan.zhihu.com/p/714128928)

[大模型的latency（延迟）和throughput（吞吐量）有什么区别？](https://blog.csdn.net/qq_25295605/article/details/141706600?spm=1001.2014.3001.5502)

## 大模型部署

**[⬆ 一键返回目录](#目录)**

[10分钟私有化部署大模型到本地](https://zhuanlan.zhihu.com/p/714669680)

## 大模型应用

**[⬆ 一键返回目录](#目录)**

[Langchain向左，扣子向右](https://blog.csdn.net/qq_25295605/article/details/141397147?spm=1001.2014.3001.5502)

[大模型output token为什么比input token贵？](https://zhuanlan.zhihu.com/p/715121827)

[大模型落地难点之结构化输出](https://zhuanlan.zhihu.com/p/714961812)

[大模型落地难点之输出的不确定性](https://blog.csdn.net/qq_25295605/article/details/141332480)

[大模型落地难点之幻觉](https://blog.csdn.net/qq_25295605/article/details/141397248?spm=1001.2014.3001.5502)

[CRUD/ETL工程师的末日?从NL2SQL到ChatBI](https://blog.csdn.net/qq_25295605/article/details/141436495?spm=1001.2014.3001.5502)


## 显卡知识

**[⬆ 一键返回目录](#目录)**

[AGI时代人人都可以看懂的显卡知识](https://mp.weixin.qq.com/s?__biz=MzkyOTY4Mjc4MQ==&mid=2247484001&idx=1&sn=5a178a9006cc308f2e84b5a0db6994ff&chksm=c2048653f5730f45b3b08af03023aee24969d89ad5586e4e25c68b09393bf5a8abfd9670a6f3&token=607452854&lang=zh_CN#rd)

## 时事热点

[Llama 3.1 405B 为什么这么大？](https://mp.weixin.qq.com/s?__biz=MzkyOTY4Mjc4MQ==&mid=2247483782&idx=1&sn=3a14a0cde14eb6643beaeb5b472ffa26&chksm=c20485b4f5730ca2d7b002a29e617a75c08d004a1b3da891ab352cbe31ca37541a546e29abc7&token=607452854&lang=zh_CN#rd)

[9.11大于9.9？大模型怎么又翻车了？](https://mp.weixin.qq.com/s?__biz=MzkyOTY4Mjc4MQ==&mid=2247483800&idx=1&sn=48b326352c37d686f7f46ee5df9f00b4&chksm=c20485aaf5730cbca8f0dfcb9746830229b8f07eec092e0e124bc558d1073ee32e3f55716221&token=607452854&lang=zh_CN#rd)