## 1. 导入


## 2. 不同的公司使用的术语不同：
- 
>

## 3. 理论的大模型推理速度

> 1


***太慢了！***

| 策略 | 难度| 数据要求|准确性提升|
| :--- |:----:| :----: |---: |
| Prompt engineering|低|无| 26%   |
| Self-reflection |低| 无|26-40% |
| Few-shot learning (with RAG)|中|少量|50% |
| Instruction Fine-tuning |高|中等|40-60%|

## 参考

<div id="refer-anchor-1"></div>

[1] [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)

[2] [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)

[3] [GitHub: LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody)



