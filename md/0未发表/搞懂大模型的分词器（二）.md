搞懂大模型的分词器（二）：BPE (Byte-Pair Encoding)

## BPE (Byte-Pair Encoding)

字节对编码 (BPE) 最初是作为一种压缩文本的算法开发的，最早是由Philip Gage于1994年在《A New Algorithm for Data Compression》一文中提出，后来被 OpenAI 在预训练 GPT 模型时用于分词器（Tokenizer）。它被许多 Transformer 模型使用，包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。

> 0

本文尝试用最直观的语言和示例来解释 BPE 算法。

本文的分词是在英文（拉丁语系）状态下进行的，中文状态下的分词会在后续的文章中讨论。


## 1. 直觉式理解

假设我们有一份语料，其中包含以下单词：

```plaintext
faster: 8, higher:6, stronger:7
```
其中，数字表示单词出现的次数。


首先，我们将其中的每个字符作为一个 token，得到的 token 如下：
```plaintext
f a s t e r: 8, h i g h e r: 8, s t r o n g e r: 8
```

对应的字典如下：
```plaintext
'a', 'e', 'f', 'g', 'h', 'i', 'n', 'o', 'r', 's', 't'
```

第二步，我们统计每两个token相邻出现的次数，得到如下结果：

```plaintext
'fa':8,'as':8,'st':15,'te':8,'er':21,'hi':6,'ig':6,'gh':6,'he':6,'tr':7,'ro':7,'on':7,'ng':7,'ge':7
```

我们将出现次数最多的字符'e'和'r'对合并'er'【这就是byte pair 字节对的名称由来】，token变为：

```plaintext
f a s t er: 8, h i g h er: 8, s t r o n g er: 8
```
对应的字典变化为：
```plaintext
'a', 'f', 'g', 'h', 'i', 'n', 'o', 's','r', 't', 'er'
```

> 注意： 此时的'e'被'er'消融了，因为在token中除了'er'中有'e'，其他地方都没有'e'了,但是'r'还在。

第三步，现在'er'已经是一个token了，我们继续统计相邻token出现的次数，得到如下结果：

```plaintext
'fa':8,'as':8,'st':15,'ter':8,'hi':6,'ig':6,'gh':6,'her':6,'tr':7,'ro':7,'on':7,'ng':7,'ger':7
```

我们将出现次数最多的字符't'和'er'对合并'ter'，token变为：

```plaintext
f a s ter: 8, h i g h er: 8, s t r o n g er: 8
```
对应的字典变化为：
```plaintext
'a', 'f', 'g', 'h', 'i', 'n', 'o', 's','r', 't', 'er', 'ter'
```

> 注意： 此时的'er'和't'都没有被'ter'消融了，因为在token中除了'ter'中有'er'，其他地方也有'er'和't'

重复上述步骤，直到达到预设的token数量或者达到预设的迭代次数；

这两个就是BPE算法的超参数，可以根据实际情况调整。

搞清楚了BPE，后续我们再来看wordpiece和sentencepiece。

## 参考

[1] [A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)

[2] [wiki:BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) 

[3] [Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)

## 欢迎关注我的GitHub和微信公众号，来不及解释了，快上船！

[GitHub: LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody)

仓库上有原始的Markdown文件，完全开源，欢迎大家Star和Fork！