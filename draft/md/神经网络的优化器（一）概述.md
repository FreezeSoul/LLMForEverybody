神经网络的优化器（一）概述

## 1. 导入

- SGD 1951 Momentum SGD 1980-1990  ASGD 1992
-  Rprop  Martin Riedmiller and Heinrich Braun. A direct adaptive method for faster backpropagation learning: The Rprop algorithm. Proceedings of the IEEE International Conference on Neural Networks, 586-591, IEEE Press, 1993  https://en.wikipedia.org/wiki/Rprop
-  AdaGrad 2011 AdaDelta2012 
- RMSprop2012
-  Adam 2014 SparseAdam Adamax
- Nadam  Nestrov-accelerated Adaptive Moment Estimation 2016 NAdam Incorporating Nesterov Momentum into Adam  2016
https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ

- AdamW 2019

- RAdam On the Variance of the Adaptive Learning Rate and Beyond 2019













## 参考

<div id="refer-anchor-1"></div>

## 欢迎关注我的GitHub和微信公众号，来不及解释了，快上船！

[GitHub: LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody)

仓库上有原始的Markdown文件，完全开源，欢迎大家Star和Fork！